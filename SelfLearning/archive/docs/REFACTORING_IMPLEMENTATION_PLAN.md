# üîß Refactoring Implementation Plan: 5 ‚Üí 2 Services

**Date**: October 13, 2025  
**Goal**: Consolidate 5 microservices into 2 optimized services  
**Expected Improvement**: 340ms latency reduction, 60% cost savings

---

## üìä Current State

**Services Running**:
1. ‚úÖ LLM Service (Port 8002) - 7 endpoints
2. ‚úÖ Web Crawler Service (Port 8003) - 7 endpoints
3. ‚úÖ Vector DB Service (Port 8004) - 10+ endpoints
4. ‚è≠Ô∏è Question Service (planned)
5. ‚è≠Ô∏è API Gateway (planned)

**Issues**:
- Too many network hops (4-5 per request)
- Services 3 & 4 are just MongoDB wrappers
- High operational complexity

---

## üéØ Target State

**Services**:
1. **API Gateway / BFF** (Port 8001)
   - Request routing
   - Auth & rate limiting
   - Response caching
   - Request aggregation

2. **Content Processing Service** (Port 8005)
   - Blog crawling
   - LLM integration (as library)
   - MongoDB operations
   - Search & vectors
   - Pipeline orchestration

---

## üìã Phase 1: Content Processing Service

### **Step 1.1: Core Configuration**

Create `content_processing_service/core/config.py`:

```python
"""Configuration for Content Processing Service."""
import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class Settings:
    """Service configuration."""
    
    # Service
    service_name: str = "content-processing-service"
    service_version: str = "1.0.0"
    port: int = 8005
    
    # MongoDB
    mongodb_url: str = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
    mongodb_database: str = os.getenv("MONGODB_DATABASE", "blog_qa_db")
    mongodb_max_pool_size: int = 100
    
    # LLM (OpenAI)
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    llm_model: str = "gpt-3.5-turbo"
    llm_temperature: float = 0.7
    llm_max_tokens: int = 2000
    
    # Crawler
    crawler_timeout: int = 30
    crawler_max_retries: int = 3
    crawler_user_agent: str = "ContentBot/1.0"
    
    # Processing
    async_processing: bool = True
    max_parallel_llm_calls: int = 3
    
    # Collections
    blogs_collection: str = "raw_blog_content"
    questions_collection: str = "processed_questions"
    summaries_collection: str = "blog_summaries"

settings = Settings()
```

### **Step 1.2: Database Manager**

Create `content_processing_service/data/database.py`:

```python
"""MongoDB connection manager."""
import logging
from typing import Optional
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase
from pymongo.errors import ServerSelectionTimeoutError

from ..core.config import settings

logger = logging.getLogger(__name__)

class DatabaseManager:
    """Manages MongoDB connections."""
    
    def __init__(self):
        self._client: Optional[AsyncIOMotorClient] = None
        self._database: Optional[AsyncIOMotorDatabase] = None
        self._is_connected = False
    
    async def connect(self):
        """Connect to MongoDB."""
        try:
            self._client = AsyncIOMotorClient(
                settings.mongodb_url,
                maxPoolSize=settings.mongodb_max_pool_size
            )
            await self._client.admin.command('ping')
            self._database = self._client[settings.mongodb_database]
            self._is_connected = True
            logger.info(f"‚úÖ Connected to MongoDB: {settings.mongodb_database}")
        except ServerSelectionTimeoutError as e:
            logger.error(f"‚ùå MongoDB connection failed: {e}")
            raise
    
    async def disconnect(self):
        """Disconnect from MongoDB."""
        if self._client:
            self._client.close()
            self._is_connected = False
            logger.info("Disconnected from MongoDB")
    
    @property
    def database(self):
        """Get database instance."""
        if not self._database:
            raise RuntimeError("Database not connected")
        return self._database
    
    @property
    def is_connected(self):
        """Check connection status."""
        return self._is_connected

# Global instance
db_manager = DatabaseManager()
```

### **Step 1.3: Blog Processing Pipeline**

Create `content_processing_service/services/pipeline_service.py`:

```python
"""Blog processing pipeline - orchestrates the entire flow."""
import asyncio
import logging
from typing import Dict, Any, List
from datetime import datetime

from ..core.config import settings
from ..data.database import db_manager
from .crawler_service import CrawlerService
from .llm_service import LLMService
from .storage_service import StorageService

logger = logging.getLogger(__name__)

class PipelineService:
    """Orchestrates blog processing pipeline."""
    
    def __init__(self):
        self.crawler = CrawlerService()
        self.llm = LLMService()
        self.storage = StorageService()
    
    async def process_blog(self, url: str) -> Dict[str, Any]:
        """
        Process a blog through the entire pipeline.
        
        Steps:
        1. Crawl URL ‚Üí get content
        2. Parallel LLM operations:
           - Generate summary
           - Generate questions
           - Generate embeddings
        3. Store all results in MongoDB
        4. Return status
        """
        try:
            logger.info(f"Starting pipeline for: {url}")
            start_time = datetime.utcnow()
            
            # Step 1: Crawl content
            logger.info("Step 1: Crawling content...")
            crawl_result = await self.crawler.crawl(url)
            content = crawl_result['content']
            title = crawl_result.get('title', '')
            
            # Step 2: Parallel LLM operations ‚úÖ KEY OPTIMIZATION
            logger.info("Step 2: Parallel LLM processing...")
            summary, questions, embedding = await asyncio.gather(
                self.llm.generate_summary(content),
                self.llm.generate_questions(content, num_questions=5),
                self.llm.generate_embedding(content),
                return_exceptions=True  # Don't fail entire pipeline
            )
            
            # Step 3: Store everything (single service, fast)
            logger.info("Step 3: Storing results...")
            blog_id = await self.storage.store_blog(
                url=url,
                title=title,
                content=content,
                metadata=crawl_result.get('metadata', {})
            )
            
            await self.storage.store_summary(
                blog_id=blog_id,
                summary=summary if not isinstance(summary, Exception) else "",
                embedding=embedding if not isinstance(embedding, Exception) else []
            )
            
            if not isinstance(questions, Exception):
                await self.storage.store_questions(
                    blog_id=blog_id,
                    blog_url=url,
                    questions=questions
                )
            
            # Calculate timing
            end_time = datetime.utcnow()
            duration_ms = (end_time - start_time).total_seconds() * 1000
            
            logger.info(f"‚úÖ Pipeline complete for {url} in {duration_ms:.0f}ms")
            
            return {
                "status": "success",
                "blog_id": blog_id,
                "url": url,
                "duration_ms": duration_ms,
                "summary_generated": not isinstance(summary, Exception),
                "questions_generated": not isinstance(questions, Exception),
                "embedding_generated": not isinstance(embedding, Exception)
            }
            
        except Exception as e:
            logger.error(f"Pipeline failed for {url}: {e}")
            return {
                "status": "failed",
                "url": url,
                "error": str(e)
            }
    
    async def get_blog_questions(self, blog_url: str) -> List[Dict]:
        """Get questions for a blog URL (fast read path)."""
        return await self.storage.get_questions_by_url(blog_url)
    
    async def get_similar_blogs(
        self,
        blog_url: str,
        limit: int = 3
    ) -> List[Dict]:
        """Get similar blogs using vector search."""
        # Get embedding for current blog
        embedding = await self.storage.get_embedding_by_url(blog_url)
        if not embedding:
            return []
        
        # Search similar (all in one service!)
        return await self.storage.vector_search(embedding, limit)
```

### **Step 1.4: Crawler Service (Internal)**

Create `content_processing_service/services/crawler_service.py`:

```python
"""Web crawling service - internal to Content Processing."""
import aiohttp
import logging
from typing import Dict, Any
from bs4 import BeautifulSoup

from ..core.config import settings

logger = logging.getLogger(__name__)

class CrawlerService:
    """Handles web crawling operations."""
    
    async def crawl(self, url: str) -> Dict[str, Any]:
        """Crawl a URL and extract content."""
        timeout = aiohttp.ClientTimeout(total=settings.crawler_timeout)
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                html = await response.text()
                
                # Parse with BeautifulSoup
                soup = BeautifulSoup(html, 'html.parser')
                
                # Extract title
                title = soup.find('title')
                title = title.text if title else ""
                
                # Extract main content
                # Remove scripts, styles
                for script in soup(["script", "style"]):
                    script.decompose()
                
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                content = '\n'.join(line for line in lines if line)
                
                return {
                    "url": url,
                    "title": title,
                    "content": content,
                    "status_code": response.status,
                    "metadata": {
                        "content_type": response.content_type,
                        "word_count": len(content.split())
                    }
                }
```

### **Step 1.5: LLM Service (Internal Library)**

Create `content_processing_service/services/llm_service.py`:

```python
"""LLM operations - internal library (not a separate service)."""
import openai
from openai import AsyncOpenAI
import logging
from typing import List, Dict

from ..core.config import settings

logger = logging.getLogger(__name__)

class LLMService:
    """Handles LLM operations as internal library."""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.openai_api_key)
    
    async def generate_summary(self, content: str) -> str:
        """Generate blog summary."""
        response = await self.client.chat.completions.create(
            model=settings.llm_model,
            messages=[{
                "role": "user",
                "content": f"Summarize this blog post in 2-3 sentences:\n\n{content[:2000]}"
            }],
            temperature=settings.llm_temperature,
            max_tokens=200
        )
        return response.choices[0].message.content
    
    async def generate_questions(
        self,
        content: str,
        num_questions: int = 5
    ) -> List[Dict[str, str]]:
        """Generate Q&A pairs."""
        response = await self.client.chat.completions.create(
            model=settings.llm_model,
            messages=[{
                "role": "user",
                "content": f"""Generate {num_questions} question-answer pairs from this content.
                
Content: {content[:3000]}

Format:
Q: question here
A: answer here

Questions:"""
            }],
            temperature=settings.llm_temperature,
            max_tokens=800
        )
        
        # Parse Q&A pairs
        text = response.choices[0].message.content
        questions = []
        lines = text.strip().split('\n')
        current_q = None
        
        for line in lines:
            line = line.strip()
            if line.startswith('Q:'):
                current_q = line[2:].strip()
            elif line.startswith('A:') and current_q:
                questions.append({
                    "question": current_q,
                    "answer": line[2:].strip()
                })
                current_q = None
        
        return questions[:num_questions]
    
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate text embedding."""
        response = await self.client.embeddings.create(
            input=text[:8000],  # Limit length
            model="text-embedding-ada-002"
        )
        return response.data[0].embedding
```

### **Step 1.6: Storage Service (Internal)**

Create `content_processing_service/services/storage_service.py`:

```python
"""MongoDB storage operations - all in one place."""
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

from ..data.database import db_manager
from ..core.config import settings

logger = logging.getLogger(__name__)

class StorageService:
    """Handles all MongoDB operations."""
    
    async def store_blog(
        self,
        url: str,
        title: str,
        content: str,
        metadata: Dict
    ) -> str:
        """Store blog content."""
        collection = db_manager.database[settings.blogs_collection]
        
        doc = {
            "url": url,
            "title": title,
            "content": content,
            "metadata": metadata,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        
        result = await collection.insert_one(doc)
        return str(result.inserted_id)
    
    async def store_summary(
        self,
        blog_id: str,
        summary: str,
        embedding: List[float]
    ):
        """Store blog summary and embedding."""
        collection = db_manager.database[settings.summaries_collection]
        
        doc = {
            "blog_id": blog_id,
            "summary": summary,
            "embedding": embedding,
            "created_at": datetime.utcnow()
        }
        
        await collection.insert_one(doc)
    
    async def store_questions(
        self,
        blog_id: str,
        blog_url: str,
        questions: List[Dict]
    ):
        """Store Q&A pairs."""
        collection = db_manager.database[settings.questions_collection]
        
        docs = [{
            "blog_id": blog_id,
            "blog_url": blog_url,
            "question": q["question"],
            "answer": q["answer"],
            "created_at": datetime.utcnow()
        } for q in questions]
        
        if docs:
            await collection.insert_many(docs)
    
    async def get_questions_by_url(self, blog_url: str) -> List[Dict]:
        """Get questions for a URL (read path)."""
        collection = db_manager.database[settings.questions_collection]
        
        cursor = collection.find({"blog_url": blog_url})
        docs = await cursor.to_list(length=100)
        
        return [{
            "question_id": str(doc["_id"]),
            "question": doc["question"],
            "answer": doc["answer"]
        } for doc in docs]
    
    async def get_embedding_by_url(self, blog_url: str) -> Optional[List[float]]:
        """Get embedding for a blog."""
        # First get blog_id
        blogs = db_manager.database[settings.blogs_collection]
        blog = await blogs.find_one({"url": blog_url})
        
        if not blog:
            return None
        
        # Get embedding
        summaries = db_manager.database[settings.summaries_collection]
        summary = await summaries.find_one({"blog_id": str(blog["_id"])})
        
        return summary.get("embedding") if summary else None
    
    async def vector_search(
        self,
        query_embedding: List[float],
        limit: int = 3
    ) -> List[Dict]:
        """Perform vector similarity search."""
        # MongoDB Atlas Vector Search or pure Python cosine similarity
        collection = db_manager.database[settings.summaries_collection]
        
        # For now, simple aggregation
        # In production, use MongoDB Atlas Vector Search
        cursor = collection.find().limit(limit * 3)
        docs = await cursor.to_list(length=limit * 3)
        
        # Calculate cosine similarity
        import numpy as np
        
        def cosine_similarity(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
        
        results = []
        for doc in docs:
            if "embedding" in doc and doc["embedding"]:
                similarity = cosine_similarity(query_embedding, doc["embedding"])
                results.append({
                    "blog_id": doc["blog_id"],
                    "similarity": float(similarity),
                    "summary": doc.get("summary", "")
                })
        
        # Sort by similarity
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:limit]
```

---

## üìã Phase 2: API Endpoints

### **Step 2.1: Create FastAPI App**

Create `content_processing_service/api/main.py`:

```python
"""Main FastAPI application."""
import logging
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ..data.database import db_manager
from .routers import processing_router, questions_router, search_router

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup/shutdown lifecycle."""
    # Startup
    await db_manager.connect()
    logger.info("‚úÖ Content Processing Service started")
    
    yield
    
    # Shutdown
    await db_manager.disconnect()
    logger.info("‚úÖ Content Processing Service stopped")

app = FastAPI(
    title="Content Processing Service",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Register routers
app.include_router(processing_router.router, prefix="/api/v1", tags=["processing"])
app.include_router(questions_router.router, prefix="/api/v1", tags=["questions"])
app.include_router(search_router.router, prefix="/api/v1", tags=["search"])

@app.get("/")
async def root():
    return {
        "service": "Content Processing Service",
        "version": "1.0.0",
        "status": "operational"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy" if db_manager.is_connected else "unhealthy",
        "database": "connected" if db_manager.is_connected else "disconnected"
    }
```

### **Step 2.2: Processing Router**

Create `content_processing_service/api/routers/processing_router.py`:

```python
"""Blog processing endpoints."""
from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel

from ...services.pipeline_service import PipelineService

router = APIRouter()
pipeline = PipelineService()

class ProcessBlogRequest(BaseModel):
    url: str
    async_mode: bool = True

@router.post("/process")
async def process_blog(
    request: ProcessBlogRequest,
    background_tasks: BackgroundTasks
):
    """Process a blog URL through the pipeline."""
    if request.async_mode:
        # Queue for background processing
        background_tasks.add_task(pipeline.process_blog, request.url)
        return {
            "status": "queued",
            "url": request.url,
            "message": "Processing in background"
        }
    else:
        # Process synchronously
        result = await pipeline.process_blog(request.url)
        return result
```

### **Step 2.3: Questions Router**

Create `content_processing_service/api/routers/questions_router.py`:

```python
"""Questions endpoints."""
from fastapi import APIRouter, Query
from typing import List, Dict

from ...services.pipeline_service import PipelineService

router = APIRouter()
pipeline = PipelineService()

@router.get("/questions")
async def get_questions(
    blog_url: str = Query(..., description="Blog URL")
) -> List[Dict]:
    """Get questions for a blog URL."""
    questions = await pipeline.get_blog_questions(blog_url)
    return questions
```

---

## üìã Phase 3: Deployment

### **Step 3.1: Docker Compose**

Create `docker-compose.yml`:

```yaml
version: '3.8'

services:
  # API Gateway / BFF
  api-gateway:
    build: ./blog_manager
    ports:
      - "8001:8001"
    environment:
      - MONGODB_URL=mongodb://mongodb:27017
      - REDIS_URL=redis://redis:6379
      - CONTENT_SERVICE_URL=http://content-service:8005
    depends_on:
      - redis
      - mongodb
      - content-service
  
  # Content Processing Service (NEW - consolidation)
  content-service:
    build: ./content_processing_service
    ports:
      - "8005:8005"
    environment:
      - MONGODB_URL=mongodb://mongodb:27017
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - mongodb
  
  # Redis for caching
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  # MongoDB
  mongodb:
    image: mongo:7.0
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db

volumes:
  mongodb_data:
```

---

## üìä Testing & Benchmarking

### **Latency Comparison Script**

Create `benchmark_architecture.py`:

```python
"""Compare latency: 5-service vs 2-service."""
import asyncio
import time
import aiohttp

async def benchmark_old_architecture(url):
    """5-service architecture."""
    start = time.time()
    
    # API Gateway ‚Üí Crawler
    await asyncio.sleep(0.05)  # Network
    await asyncio.sleep(0.5)   # Crawl
    
    # Crawler ‚Üí Vector DB
    await asyncio.sleep(0.05)  # Network
    await asyncio.sleep(0.02)  # DB
    
    # API Gateway ‚Üí LLM
    await asyncio.sleep(0.05)  # Network
    await asyncio.sleep(2.0)   # LLM
    
    # LLM ‚Üí Vector DB
    await asyncio.sleep(0.05)  # Network
    
    # API Gateway ‚Üí Questions
    await asyncio.sleep(0.05)  # Network
    
    return time.time() - start

async def benchmark_new_architecture(url):
    """2-service architecture."""
    start = time.time()
    
    # API Gateway ‚Üí Content Service
    await asyncio.sleep(0.01)  # Internal
    
    # Content Service (all internal):
    await asyncio.sleep(0.5)   # Crawl
    # Parallel LLM operations
    await asyncio.gather(
        asyncio.sleep(1.0),    # Summary
        asyncio.sleep(1.0),    # Questions
        asyncio.sleep(0.5)     # Embeddings
    )  # Total: 1.0s (not 2.5s!)
    await asyncio.sleep(0.02)  # MongoDB
    
    return time.time() - start

async def main():
    old = await benchmark_old_architecture("https://example.com")
    new = await benchmark_new_architecture("https://example.com")
    
    print(f"Old (5-service): {old*1000:.0f}ms")
    print(f"New (2-service): {new*1000:.0f}ms")
    print(f"Improvement: {(old-new)*1000:.0f}ms ({(1-new/old)*100:.1f}% faster)")

asyncio.run(main())
```

---

## üìù Migration Checklist

- [ ] Create Content Processing Service structure
- [ ] Implement pipeline with parallel LLM calls
- [ ] Add all internal services (crawler, LLM, storage)
- [ ] Create API endpoints
- [ ] Update API Gateway to call new service
- [ ] Add Redis caching
- [ ] Create Docker Compose
- [ ] Run benchmarks
- [ ] Load testing
- [ ] Update monitoring dashboards
- [ ] Deploy to staging
- [ ] A/B test both architectures
- [ ] Deploy to production
- [ ] Decomission old services

---

## üéØ Success Criteria

- ‚úÖ Latency < 2600ms (current: 2870ms)
- ‚úÖ Services reduced from 5 to 2
- ‚úÖ All tests passing
- ‚úÖ Cost reduced by 50%+
- ‚úÖ Deployment simplified
- ‚úÖ Monitoring & alerts working

---

**This is a comprehensive refactoring that will significantly improve your system. Start with Phase 1, test thoroughly, then proceed to Phase 2 and 3.**

