#!/usr/bin/env python3
"""
Simple CLI interface for the LLM service.

This script provides a command-line interface to interact with the LLM service,
allowing users to send prompts and receive responses from various LLM providers.

Usage:
    python llm_chat.py "Your prompt here"
    python llm_chat.py "Your prompt" --provider openai --model gpt-4
    python llm_chat.py "Your prompt" --stream
"""

import asyncio
import argparse
import sys
import os
from typing import Optional

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from llm_service import (
    LLMService, LLMProvider, LLMServiceError, 
    LLMAuthenticationError, LLMConfigurationError
)


async def chat_with_llm(
    prompt: str,
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    stream: bool = False,
    system_prompt: Optional[str] = None
):
    """Chat with the LLM service."""
    try:
        # Initialize service
        print("üîÑ Initializing LLM service...")
        service = LLMService()
        await service.initialize()
        
        # Convert provider string to enum
        provider_enum = None
        if provider:
            try:
                provider_enum = LLMProvider(provider.lower())
            except ValueError:
                available = [p.value for p in LLMProvider]
                print(f"‚ùå Invalid provider '{provider}'. Available: {', '.join(available)}")
                return
        
        # Show configuration
        available_providers = service.get_available_providers()
        if not available_providers:
            print("‚ùå No LLM providers available. Please configure API keys.")
            print("   Set OPENAI_API_KEY or ANTHROPIC_API_KEY environment variables.")
            return
        
        print(f"‚úÖ Available providers: {', '.join(available_providers)}")
        
        # Determine which provider and model to use
        actual_provider = provider or service.config.default_provider
        if actual_provider not in available_providers:
            actual_provider = available_providers[0]
            print(f"‚ö†Ô∏è  Requested provider not available, using: {actual_provider}")
        
        # Get default model if not specified
        if not model:
            provider_enum = LLMProvider(actual_provider)
            available_models = service.get_provider_models(provider_enum)
            model = available_models[0] if available_models else "default"
        
        print(f"ü§ñ Using: {actual_provider} / {model}")
        print(f"üìù Prompt: {prompt}")
        print("=" * 60)
        
        if stream:
            # Streaming response
            print("üåä Streaming response:")
            async for chunk in service.stream_generate(
                prompt=prompt,
                provider=provider_enum,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt
            ):
                print(chunk, end="", flush=True)
            print("\n")
        else:
            # Regular response
            print("ü§ñ Response:")
            response = await service.generate(
                prompt=prompt,
                provider=provider_enum,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt
            )
            
            print(response.content)
            
            # Show usage stats if available
            if response.usage:
                print(f"\nüìä Usage: {response.usage}")
            
            print(f"\n‚úÖ Response generated by {response.provider}/{response.model}")
    
    except LLMAuthenticationError as e:
        print(f"üîê Authentication Error: {e}")
        print("   Please check your API keys in environment variables.")
    except LLMConfigurationError as e:
        print(f"‚öôÔ∏è  Configuration Error: {e}")
    except LLMServiceError as e:
        print(f"üö® LLM Service Error: {e}")
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Chat with LLM service",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python llm_chat.py "What is artificial intelligence?"
  python llm_chat.py "Explain quantum computing" --provider anthropic
  python llm_chat.py "Tell me a story" --stream --temperature 0.9
  python llm_chat.py "Code review this function" --model gpt-4 --max-tokens 500
        """
    )
    
    parser.add_argument(
        "prompt",
        help="The prompt to send to the LLM"
    )
    
    parser.add_argument(
        "--provider", "-p",
        choices=["openai", "anthropic", "google", "ollama"],
        help="LLM provider to use (default: auto-select)"
    )
    
    parser.add_argument(
        "--model", "-m",
        help="Specific model to use (default: provider default)"
    )
    
    parser.add_argument(
        "--temperature", "-t",
        type=float,
        default=0.7,
        help="Temperature for response generation (0.0-2.0, default: 0.7)"
    )
    
    parser.add_argument(
        "--max-tokens",
        type=int,
        help="Maximum tokens in response"
    )
    
    parser.add_argument(
        "--stream", "-s",
        action="store_true",
        help="Stream the response in real-time"
    )
    
    parser.add_argument(
        "--system-prompt",
        help="System prompt to set context/behavior"
    )
    
    parser.add_argument(
        "--version",
        action="version",
        version="LLM Chat CLI 1.0.0"
    )
    
    args = parser.parse_args()
    
    # Validate temperature
    if not 0.0 <= args.temperature <= 2.0:
        print("‚ùå Temperature must be between 0.0 and 2.0")
        sys.exit(1)
    
    # Run the chat
    try:
        asyncio.run(chat_with_llm(
            prompt=args.prompt,
            provider=args.provider,
            model=args.model,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            stream=args.stream,
            system_prompt=args.system_prompt
        ))
    except KeyboardInterrupt:
        print("\nüëã Chat interrupted by user")
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
